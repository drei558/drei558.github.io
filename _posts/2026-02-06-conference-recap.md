---
layout: post
title: Recap of What I Learned at the DAM and Museums, Digital Preservation Summit 2026
tags: [Artificial Intelligence, DAM, GenAI, Freelance, Conferences]
comments: true
---

I attended Henry Stewart Events' virtual DAM and Museums/Digital Preservation Summit 2026 yesterday and here's what I learned. 

<img width="500" height="500" alt="Cartoony Crow with their face pressed against window glass. Note taped to the window reads: Conference recap: DAM and Museums 2026" src="https://github.com/user-attachments/assets/d8448da6-e66c-441d-b048-39819ac8ca56" />

Most talks were on how cultural heritage institutions are using GenAI models trained on the specific institution's data to: 
- catalog subjects
- identify people in photos
- get better OCR (optical character recognition; this is the tech that takes "words" in like a PDF and outputs them as text) for cursive handwriting on historical documents

The Theodore Roosevelt Presidential Library project really interested me. They worked with Terentia to build a MyGPT LLM with Microsoft Azure to help make GenAI summaries of historical docs, transcribe for OCR, and they also discussed GenAI audio (e.g. making President Roosevelt "speak" through GenAI voice). The talk framed GenAI tech as accessibility, as a way to increase access/views to younger audiences, and as a method to manage the archival/data/tagging backlog that all institutions have. These ideas were also echoed in other sessions related to how they said they use GenAI.

The conference unfortunately didn't cover broader systemic environmental, cybersecurity, ethical/professional value-driven, political, or economic issues (e.g. job loss) related to GenAI. 

I'm interested in seeing more about how DAM work - which isn't limited to any sector/country/job title - will handle data privacy and cybersecurity issues that arise form feeding institutional data into GenAI: 
- GenAI opens DAM labor up to big issues like ransomware, cyberattacks, data leaks, and physical computing costs. IT/computer science is already not usually integrated in DAM work. 
- There was a lot of emphasis on how humans quality check GenAI output, but that adds hundreds of hours of additional labor, often correcting incorrect/mistaken data. 
- It also struck me as odd that many talks pointed out they have cloud-computing (so no physical server on their premise) but that just means the physical server is somewhere else in the world. Cloud-computing doesn't divorce us from data centers. It makes us dependent on them. 
- Data protection and privacy laws and regulations (GDPR, Indigenous data sovereignty mandates, EU AI Act, HIPAA, copyright law, FOIA, open access policies) don't yet know how to handle GenAI data like this. Plus cultural heritage institutions/DAM operate under international government laws/rules that conflict or undermine professional standards.
